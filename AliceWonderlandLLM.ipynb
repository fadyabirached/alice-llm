{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sVYPTzgfgyN",
        "outputId": "7f9760bb-709c-4002-be57-d7e57acf0ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Installing Python libraries... ---\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.3.6-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.48.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting ollama<1.0.0,>=0.5.1 (from langchain-ollama)\n",
            "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.1.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_ollama-0.3.6-py3-none-any.whl (24 kB)\n",
            "Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.48.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m135.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: watchdog, python-dotenv, pyngrok, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydeck, pydantic-settings, ollama, dataclasses-json, streamlit, langchain-ollama, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.12.0 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-ollama-0.3.6 marshmallow-3.26.1 mypy-extensions-1.1.0 ollama-0.5.3 pydantic-settings-2.10.1 pydeck-0.9.1 pyngrok-7.3.0 python-dotenv-1.1.1 streamlit-1.48.1 typing-inspect-0.9.0 watchdog-6.0.0\n",
            "\n",
            "--- 2. Installing Ollama... ---\n",
            "\n",
            "--- ‚úÖ Step 1 Complete: All software installed. ---\n"
          ]
        }
      ],
      "source": [
        "# Install All Libraries & Ollama\n",
        "print(\"--- 1. Installing Python libraries... ---\")\n",
        "!pip install -U langchain langchain-community langchain-ollama faiss-cpu streamlit pyngrok\n",
        "\n",
        "print(\"\\n--- 2. Installing Ollama... ---\")\n",
        "# The -q flag makes the output less verbose\n",
        "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1\n",
        "\n",
        "print(\"\\n--- ‚úÖ Step 1 Complete: All software installed. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start and Verify Ollama Server\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "print(\"--- 1. Stopping any old Ollama server processes (for a clean start)... ---\")\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"ollama\"], check=True)\n",
        "    print(\"Old Ollama processes stopped.\")\n",
        "    time.sleep(3)\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"No old Ollama processes found. Perfect.\")\n",
        "\n",
        "print(\"\\n--- 2. Starting a new Ollama server in the background... ---\")\n",
        "command = \"nohup ollama serve > ollama.log 2>&1 &\"\n",
        "os.system(command)\n",
        "print(\"Server start command issued. Waiting 20 seconds for it to initialize fully...\")\n",
        "# We wait a generous amount of time to ensure the server is ready.\n",
        "time.sleep(20)\n",
        "\n",
        "print(\"\\n--- 3. Verifying the server is running... ---\")\n",
        "# Check the log file for the \"Listening on\" message, which indicates success.\n",
        "print(\"--- Log File Content: ---\")\n",
        "!cat ollama.log\n",
        "\n",
        "# Check the system's process list to see if 'ollama serve' is an active process.\n",
        "print(\"\\n--- Active Processes: ---\")\n",
        "!ps -ef | grep ollama\n",
        "\n",
        "print(\"\\n--- ‚úÖ Step 2 Complete. Check the output above. You should see 'Listening on...' in the log and 'ollama serve' in the process list. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Sd49Y6-f3iq",
        "outputId": "2fa2e3fe-4b23-46b6-a586-640e7fbc6700"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Stopping any old Ollama server processes (for a clean start)... ---\n",
            "No old Ollama processes found. Perfect.\n",
            "\n",
            "--- 2. Starting a new Ollama server in the background... ---\n",
            "Server start command issued. Waiting 20 seconds for it to initialize fully...\n",
            "\n",
            "--- 3. Verifying the server is running... ---\n",
            "--- Log File Content: ---\n",
            "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is: \n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDR0+sS3cjnIFzCfRDPPEBqUb9vfdz7Ir7gk2U8yGvk8\n",
            "\n",
            "time=2025-08-15T09:42:30.977Z level=INFO source=routes.go:1304 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-08-15T09:42:30.977Z level=INFO source=images.go:477 msg=\"total blobs: 0\"\n",
            "time=2025-08-15T09:42:30.977Z level=INFO source=images.go:484 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-08-15T09:42:30.977Z level=INFO source=routes.go:1357 msg=\"Listening on 127.0.0.1:11434 (version 0.11.4)\"\n",
            "time=2025-08-15T09:42:30.977Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-08-15T09:42:31.280Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-f4906c79-cb2f-5389-84de-2aec9e558ccc library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
            "time=2025-08-15T09:42:31.280Z level=INFO source=routes.go:1398 msg=\"entering low vram mode\" \"total vram\"=\"14.7 GiB\" threshold=\"20.0 GiB\"\n",
            "\n",
            "--- Active Processes: ---\n",
            "root        2342       1  1 09:42 ?        00:00:00 ollama serve\n",
            "root        2453    1663  0 09:42 ?        00:00:00 /bin/bash -c ps -ef | grep ollama\n",
            "root        2455    2453  0 09:42 ?        00:00:00 grep ollama\n",
            "\n",
            "--- ‚úÖ Step 2 Complete. Check the output above. You should see 'Listening on...' in the log and 'ollama serve' in the process list. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull Models using the Python Library\n",
        "import ollama\n",
        "\n",
        "print(\"--- Starting model downloads. This will take several minutes. ---\")\n",
        "\n",
        "try:\n",
        "    # Pull the smaller, faster embedding model first as a test\n",
        "    print(\"\\n--- Pulling embedding model (mxbai-embed-large)... ---\")\n",
        "    ollama.pull('mxbai-embed-large')\n",
        "    print(\"‚úÖ Embedding model downloaded successfully.\")\n",
        "\n",
        "    # Pull the large Llama 3 model\n",
        "    print(\"\\n--- Pulling generation model (llama3)... ---\")\n",
        "    print(\"This is a large file (~4.7 GB) and will take a while. Please be patient.\")\n",
        "    ollama.pull('llama3')\n",
        "    print(\"‚úÖ Generation model downloaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå An error occurred during model pull: {e}\")\n",
        "    print(\"Please check the server log from Step 2 for more details.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Verifying all models are installed on the server... ---\")\n",
        "!ollama list\n",
        "\n",
        "print(\"\\n--- ‚úÖ Step 3 Complete. Both models should be listed above. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgtnKJmZJq09",
        "outputId": "cc9b86a8-4f9c-4654-ca60-7d4bd9279b3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting model downloads. This will take several minutes. ---\n",
            "\n",
            "--- Pulling embedding model (mxbai-embed-large)... ---\n",
            "‚úÖ Embedding model downloaded successfully.\n",
            "\n",
            "--- Pulling generation model (llama3)... ---\n",
            "This is a large file (~4.7 GB) and will take a while. Please be patient.\n",
            "‚úÖ Generation model downloaded successfully.\n",
            "\n",
            "--- Verifying all models are installed on the server... ---\n",
            "NAME                        ID              SIZE      MODIFIED               \n",
            "llama3:latest               365c0bd3c000    4.7 GB    Less than a second ago    \n",
            "mxbai-embed-large:latest    468836162de7    669 MB    54 seconds ago            \n",
            "\n",
            "--- ‚úÖ Step 3 Complete. Both models should be listed above. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the RAG Pipeline (The Setup)\n",
        "import os\n",
        "import time\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "print(\"--- Starting RAG Pipeline Setup ---\")\n",
        "\n",
        "# --- 1. Load the Document ---\n",
        "file_path = \"alice_in_wonderland.txt\"\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"FATAL ERROR: '{file_path}' not found. Please upload it and restart.\")\n",
        "else:\n",
        "    print(\"\\n--- 1. Loading Document ---\")\n",
        "    loader = TextLoader(file_path, encoding='utf-8')\n",
        "    documents = loader.load()\n",
        "\n",
        "    # --- 2. Split Document into Chunks ---\n",
        "    print(\"--- 2. Splitting Document into Chunks ---\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "    print(f\"Document split into {len(splits)} chunks.\")\n",
        "\n",
        "    # --- 3. Create Embeddings and Vector Store ---\n",
        "    print(\"--- 3. Creating Vector Store (this may take a moment)... ---\")\n",
        "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
        "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "    print(\"Vector Store created successfully.\")\n",
        "\n",
        "    # --- 4. Configure the RAG Chain ---\n",
        "    print(\"--- 4. Configuring the RAG Chain ---\")\n",
        "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8, \"fetch_k\": 20})\n",
        "    llm = ChatOllama(model=\"llama3\", temperature=0)\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "You must answer based ONLY on the provided context.\n",
        "If the answer is not contained within the text provided, you must say \"I cannot find that information in the provided text.\"\n",
        "Do not provide any information or commentary outside of the given context.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\"\"\")\n",
        "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "    print(\"\\n--- ‚úÖ Step 4 Complete: RAG Pipeline is ready. You can now run the Q&A cell below. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfGCa2BggXVz",
        "outputId": "bde44aa9-81db-48e2-d9e7-60eb8426a8da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting RAG Pipeline Setup ---\n",
            "\n",
            "--- 1. Loading Document ---\n",
            "--- 2. Splitting Document into Chunks ---\n",
            "Document split into 195 chunks.\n",
            "--- 3. Creating Vector Store (this may take a moment)... ---\n",
            "Vector Store created successfully.\n",
            "--- 4. Configuring the RAG Chain ---\n",
            "\n",
            "--- ‚úÖ Step 4 Complete: RAG Pipeline is ready. You can now run the Q&A cell below. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Questions with Debugging (The Interactive Testing Loop)\n",
        "\n",
        "# This cell uses the 'retrieval_chain' and 'retriever' variables created in the cell above.\n",
        "# You can run this cell multiple times without needing to rebuild the RAG pipeline.\n",
        "\n",
        "try:\n",
        "    retrieval_chain\n",
        "except NameError:\n",
        "    print(\"The 'retrieval_chain' is not defined. Please run the 'Build the RAG Pipeline' cell (Step 4) first.\")\n",
        "else:\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        query = input(\"Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): \")\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        if not query.strip():\n",
        "            continue\n",
        "\n",
        "        # DEBUG: See what the retriever finds\n",
        "        print(\"\\n--- DEBUG: Retrieving context... ---\")\n",
        "        retrieved_docs = retriever.invoke(query)\n",
        "        print(f\"Found {len(retrieved_docs)} chunks for the LLM.\")\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            print(f\"[CHUNK {i+1}]: {doc.page_content[:120]}...\")\n",
        "        print(\"--- END OF CONTEXT ---\")\n",
        "\n",
        "        # Get the final answer\n",
        "        print(\"\\n--- Generating final answer... ---\")\n",
        "        response = retrieval_chain.invoke({\"input\": query})\n",
        "\n",
        "        print(\"\\n\" + \"‚úÖ\" * 25)\n",
        "        print(f\"\\nFinal Answer:\\n\\n{response['answer']}\")\n",
        "        print(\"‚úÖ\" * 25)\n",
        "\n",
        "print(\"\\n--- Exiting Program. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKwPLGJrNRlU",
        "outputId": "847a8d79-0a65-4148-92d0-26610c2b67c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): Who is the president of the US\n",
            "\n",
            "--- DEBUG: Retrieving context... ---\n",
            "Found 8 chunks for the LLM.\n",
            "[CHUNK 1]: `Read them,' said the King.\n",
            "\n",
            "  The White Rabbit put on his spectacles.  `Where shall I begin,\n",
            "please your Majesty?' he a...\n",
            "[CHUNK 2]: `And now which is which?' she said to herself, and nibbled a\n",
            "little of the right-hand bit to try the effect:  the next m...\n",
            "[CHUNK 3]: `Fury said to a\n",
            "                   mouse, That he\n",
            "                 met in the\n",
            "               house,\n",
            "            \"Let us\n",
            "...\n",
            "[CHUNK 4]: CHAPTER V\n",
            "\n",
            "                    Advice from a Caterpillar\n",
            "\n",
            "\n",
            "  The Caterpillar and Alice looked at each other for some tim...\n",
            "[CHUNK 5]: `Speak roughly to your little boy,\n",
            "          And beat him when he sneezes:\n",
            "        He only does it to annoy,\n",
            "          B...\n",
            "[CHUNK 6]: `Found WHAT?' said the Duck.\n",
            "\n",
            "  `Found IT,' the Mouse replied rather crossly:  `of course you\n",
            "know what \"it\" means.'\n",
            "\n",
            "  ...\n",
            "[CHUNK 7]: If I or she should chance to be\n",
            "          Involved in this affair,\n",
            "        He trusts to you to set them free,\n",
            "          ...\n",
            "[CHUNK 8]: `But what did the Dormouse say?' one of the jury asked.\n",
            "\n",
            "  `That I can't remember,' said the Hatter.\n",
            "\n",
            "  `You MUST rememb...\n",
            "--- END OF CONTEXT ---\n",
            "\n",
            "--- Generating final answer... ---\n",
            "\n",
            "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "I cannot find that information in the provided text.\n",
            "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): What animal was sitting and grinning in the Duchess‚Äôs kitchen?\n",
            "\n",
            "--- DEBUG: Retrieving context... ---\n",
            "Found 8 chunks for the LLM.\n",
            "[CHUNK 1]: `Anything you like,' said the Footman, and began whistling.\n",
            "\n",
            "  `Oh, there's no use in talking to him,' said Alice desper...\n",
            "[CHUNK 2]: `Found WHAT?' said the Duck.\n",
            "\n",
            "  `Found IT,' the Mouse replied rather crossly:  `of course you\n",
            "know what \"it\" means.'\n",
            "\n",
            "  ...\n",
            "[CHUNK 3]: Alice was just beginning to think to herself, `Now, what am I\n",
            "to do with this creature when I get it home?' when it grun...\n",
            "[CHUNK 4]: `Soo--oop of the e--e--evening,\n",
            "        Beautiful, beautiful Soup!'\n",
            "\n",
            "\n",
            "\n",
            "                           CHAPTER XI\n",
            "\n",
            "          ...\n",
            "[CHUNK 5]: `She's in prison,' the Queen said to the executioner:  `fetch\n",
            "her here.'  And the executioner went off like an arrow.\n",
            "\n",
            " ...\n",
            "[CHUNK 6]: The long grass rustled at her feet as the White Rabbit hurried\n",
            "by--the frightened Mouse splashed his way through the\n",
            "nei...\n",
            "[CHUNK 7]: `Please would you tell me,' said Alice, a little timidly, for\n",
            "she was not quite sure whether it was good manners for her...\n",
            "[CHUNK 8]: `If that's all you know about it, you may stand down,'\n",
            "continued the King.\n",
            "\n",
            "  `I can't go no lower,' said the Hatter:  `...\n",
            "--- END OF CONTEXT ---\n",
            "\n",
            "--- Generating final answer... ---\n",
            "\n",
            "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "According to the provided context, a large cat was sitting on the hearth and grinning from ear to ear in the Duchess's kitchen.\n",
            "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): exit\n",
            "\n",
            "--- Exiting Program. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Test (Clean Output)\n",
        "\n",
        "import time\n",
        "\n",
        "# This code assumes the 'retrieval_chain' variable was created in the previous cell.\n",
        "# It performs a single, non-interactive query and prints a clean result.\n",
        "\n",
        "# Safety check to ensure the RAG pipeline has been built.\n",
        "try:\n",
        "    retrieval_chain\n",
        "except NameError:\n",
        "    print(\"FATAL ERROR: The 'retrieval_chain' is not defined. Please run the 'Build the RAG Pipeline' cell first.\")\n",
        "else:\n",
        "    # 1. Define the single question you want to ask.\n",
        "    # You can change the text inside the quotes to test other questions.\n",
        "    question = \"Why did Alice decide not to drink immediately from the bottle labeled ‚ÄúDRINK ME‚Äù?\"\n",
        "\n",
        "    print(\"--- Running a single, clean test ---\")\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "\n",
        "    # 2. Invoke the chain to get the response and measure the time.\n",
        "    start_time = time.time()\n",
        "    response = retrieval_chain.invoke({\"input\": question})\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 3. Print only the final, clean answer.\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"Final Answer:\")\n",
        "    # Access the 'answer' key from the response dictionary.\n",
        "    print(response['answer'])\n",
        "    print(\"-\"*50)\n",
        "    print(f\"(Query completed in {end_time - start_time:.2f} seconds)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nIC2H_9Ort8",
        "outputId": "0879cef7-52f3-4e8c-ae76-31a29e33733f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running a single, clean test ---\n",
            "\n",
            "Question: Why did Alice decide not to drink immediately from the bottle labeled ‚ÄúDRINK ME‚Äù?\n",
            "\n",
            "--------------------------------------------------\n",
            "Final Answer:\n",
            "According to the provided context, Alice decided not to drink immediately from the bottle labeled \"DRINK ME\" because she had read several nice little histories about children who got burnt, eaten up by wild beasts and other unpleasant things, all because they WOULD not remember the simple rules their friends had taught them. She had never forgotten that if you drink much from a bottle marked \"poison,\" it is almost certain to disagree with you, sooner or later.\n",
            "--------------------------------------------------\n",
            "(Query completed in 4.70 seconds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the app.py file\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "import os\n",
        "\n",
        "# --- CORE LOGIC ---\n",
        "# Use Streamlit's caching to load and build the question-answering system only once.\n",
        "@st.cache_resource\n",
        "def build_question_answering_system():\n",
        "    \"\"\"\n",
        "    Builds and returns a question-answering system based on the book's text.\n",
        "    The function is cached to avoid rebuilding on every app rerun.\n",
        "    \"\"\"\n",
        "    st.header(\"Getting Wonderland ready for your questions (this happens only once)...\", divider=\"rainbow\")\n",
        "\n",
        "    # Ensure the source document exists\n",
        "    file_path = \"alice_in_wonderland.txt\"\n",
        "    if not os.path.exists(file_path):\n",
        "        st.error(f\"FATAL ERROR: '{file_path}' not found. Please make sure it's available.\")\n",
        "        st.stop()\n",
        "\n",
        "    # 1. Load the Document\n",
        "    loader = TextLoader(file_path, encoding='utf-8')\n",
        "    documents = loader.load()\n",
        "\n",
        "    # 2. Split Document into Chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "\n",
        "    # 3. Create Embeddings and Vector Store\n",
        "    # This connects to the Ollama server running in the background.\n",
        "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
        "    try:\n",
        "        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error preparing the book's text: {e}. Is your Ollama server running?\")\n",
        "        return None\n",
        "\n",
        "    # 4. Configure the LLM Chain\n",
        "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8, \"fetch_k\": 20})\n",
        "    llm = ChatOllama(model=\"llama3\", temperature=0)\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "You must answer based ONLY on the provided context.\n",
        "If the answer is not contained within the text provided, you must say \"I cannot find that information in the provided text.\"\n",
        "Do not provide any information or commentary outside of the given context.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\"\"\")\n",
        "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "    st.success(\"‚úÖ The book is ready! Ask your question below.\")\n",
        "    return retrieval_chain\n",
        "\n",
        "# --- STREAMLIT UI ---\n",
        "\n",
        "st.title(\"üêá Chat with Alice in Wonderland\")\n",
        "st.info(\"Ask any question about 'Alice's Adventures in Wonderland' and get answers directly from the book's text.\")\n",
        "\n",
        "# Check for the book file before starting\n",
        "if not os.path.exists(\"alice_in_wonderland.txt\"):\n",
        "    st.warning(\"Please upload 'alice_in_wonderland.txt' to the files panel on the left to begin.\")\n",
        "    st.stop()\n",
        "\n",
        "# Build the question-answering system\n",
        "try:\n",
        "    qa_chain = build_question_answering_system()\n",
        "except Exception as e:\n",
        "    st.error(f\"An error occurred while preparing the book: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Get user input\n",
        "user_query = st.text_input(\"Ask a question:\", placeholder=\"e.g., Why did Alice follow the White Rabbit?\")\n",
        "\n",
        "if user_query:\n",
        "    with st.spinner(\"Searching for answers in Wonderland...\"):\n",
        "        if qa_chain:\n",
        "            response = qa_chain.invoke({\"input\": user_query})\n",
        "            st.header(\"Answer:\", divider=\"rainbow\")\n",
        "            st.write(response['answer'])\n",
        "\n",
        "            # (Optional) Display the context documents for technical users\n",
        "            with st.expander(\"Show Context Used\"):\n",
        "                st.write(\"This section shows the exact snippets from the book that were used to generate the answer.\")\n",
        "                st.json(response['context'])\n",
        "        else:\n",
        "            st.error(\"The question-answering system is not available. Please check the setup.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiiXiVJhDwIN",
        "outputId": "dfb7c5a4-f7c8-4465-e6f5-efd158b205aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the app\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "# 1. Get your ngrok authtoken\n",
        "print(\"Enter your ngrok authtoken. You can get it from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "authtoken = getpass.getpass()\n",
        "ngrok.set_auth_token(authtoken)\n",
        "\n",
        "# 2. Start the ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"\\n\\nüöÄ Your Streamlit app is live! Click the link below:\\n{public_url}\\n\")\n",
        "\n",
        "# 3. Run the Streamlit app\n",
        "# The '!' runs a shell command. We run streamlit and point it to our app.py file.\n",
        "# The '&' at the end is not strictly needed here as we want the cell to keep running.\n",
        "!streamlit run app.py --server.port 8501 --server.enableCORS false"
      ],
      "metadata": {
        "id": "cQ1y3rtlIVpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
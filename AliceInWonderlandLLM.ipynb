{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sVYPTzgfgyN",
        "outputId": "d099789f-9e56-4c37-a641-b279b7e2f7bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Installing Python libraries... ---\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.3.6-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting ollama<1.0.0,>=0.5.1 (from langchain-ollama)\n",
            "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_ollama-0.3.6-py3-none-any.whl (24 kB)\n",
            "Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, ollama, dataclasses-json, langchain-ollama, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.12.0 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-ollama-0.3.6 marshmallow-3.26.1 mypy-extensions-1.1.0 ollama-0.5.3 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n",
            "\n",
            "--- 2. Installing Ollama... ---\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\n",
            "--- ✅ Step 1 Complete: All software installed. ---\n"
          ]
        }
      ],
      "source": [
        "# Install All Libraries & Ollama\n",
        "print(\"--- 1. Installing Python libraries... ---\")\n",
        "!pip install -U langchain langchain-community langchain-ollama faiss-cpu\n",
        "\n",
        "print(\"\\n--- 2. Installing Ollama... ---\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"\\n--- ✅ Step 1 Complete: All software installed. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start and Verify Ollama Server\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "print(\"--- 1. Stopping any old Ollama server processes (for a clean start)... ---\")\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"ollama\"], check=True)\n",
        "    print(\"Old Ollama processes stopped.\")\n",
        "    time.sleep(3)\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"No old Ollama processes found. Perfect.\")\n",
        "\n",
        "print(\"\\n--- 2. Starting a new Ollama server in the background... ---\")\n",
        "command = \"nohup ollama serve > ollama.log 2>&1 &\"\n",
        "os.system(command)\n",
        "print(\"Server start command issued. Waiting 20 seconds for it to initialize fully...\")\n",
        "# We wait a generous amount of time to ensure the server is ready.\n",
        "time.sleep(20)\n",
        "\n",
        "print(\"\\n--- 3. Verifying the server is running... ---\")\n",
        "# Check the log file for the \"Listening on\" message, which indicates success.\n",
        "print(\"--- Log File Content: ---\")\n",
        "!cat ollama.log\n",
        "\n",
        "# Check the system's process list to see if 'ollama serve' is an active process.\n",
        "print(\"\\n--- Active Processes: ---\")\n",
        "!ps -ef | grep ollama\n",
        "\n",
        "print(\"\\n--- ✅ Step 2 Complete. Check the output above. You should see 'Listening on...' in the log and 'ollama serve' in the process list. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Sd49Y6-f3iq",
        "outputId": "75d02d53-8cbd-4882-97ce-4d3615b53209"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Stopping any old Ollama server processes (for a clean start)... ---\n",
            "No old Ollama processes found. Perfect.\n",
            "\n",
            "--- 2. Starting a new Ollama server in the background... ---\n",
            "Server start command issued. Waiting 20 seconds for it to initialize fully...\n",
            "\n",
            "--- 3. Verifying the server is running... ---\n",
            "--- Log File Content: ---\n",
            "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is: \n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAo+GS8lErQtpxK+6VbVQmKzcIXD0PZbCknDZTfXTMkY\n",
            "\n",
            "time=2025-08-15T07:57:11.270Z level=INFO source=routes.go:1304 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-08-15T07:57:11.270Z level=INFO source=images.go:477 msg=\"total blobs: 0\"\n",
            "time=2025-08-15T07:57:11.271Z level=INFO source=images.go:484 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-08-15T07:57:11.271Z level=INFO source=routes.go:1357 msg=\"Listening on 127.0.0.1:11434 (version 0.11.4)\"\n",
            "time=2025-08-15T07:57:11.271Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-08-15T07:57:11.484Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-ac40cae1-0b81-c7f8-85d0-024c8a93bce1 library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
            "time=2025-08-15T07:57:11.484Z level=INFO source=routes.go:1398 msg=\"entering low vram mode\" \"total vram\"=\"14.7 GiB\" threshold=\"20.0 GiB\"\n",
            "\n",
            "--- Active Processes: ---\n",
            "root         620       1  0 07:57 ?        00:00:00 ollama serve\n",
            "root         733     197  0 07:57 ?        00:00:00 /bin/bash -c ps -ef | grep ollama\n",
            "root         735     733  0 07:57 ?        00:00:00 grep ollama\n",
            "\n",
            "--- ✅ Step 2 Complete. Check the output above. You should see 'Listening on...' in the log and 'ollama serve' in the process list. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull Models using the Python Library\n",
        "import ollama\n",
        "\n",
        "print(\"--- Starting model downloads. This will take several minutes. ---\")\n",
        "\n",
        "try:\n",
        "    # Pull the smaller, faster embedding model first as a test\n",
        "    print(\"\\n--- Pulling embedding model (mxbai-embed-large)... ---\")\n",
        "    ollama.pull('mxbai-embed-large')\n",
        "    print(\"✅ Embedding model downloaded successfully.\")\n",
        "\n",
        "    # Pull the large Llama 3 model\n",
        "    print(\"\\n--- Pulling generation model (llama3)... ---\")\n",
        "    print(\"This is a large file (~4.7 GB) and will take a while. Please be patient.\")\n",
        "    ollama.pull('llama3')\n",
        "    print(\"✅ Generation model downloaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An error occurred during model pull: {e}\")\n",
        "    print(\"Please check the server log from Step 2 for more details.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Verifying all models are installed on the server... ---\")\n",
        "!ollama list\n",
        "\n",
        "print(\"\\n--- ✅ Step 3 Complete. Both models should be listed above. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgtnKJmZJq09",
        "outputId": "2210039a-227e-4b87-c0ad-e4554b1cec39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting model downloads. This will take several minutes. ---\n",
            "\n",
            "--- Pulling embedding model (mxbai-embed-large)... ---\n",
            "✅ Embedding model downloaded successfully.\n",
            "\n",
            "--- Pulling generation model (llama3)... ---\n",
            "This is a large file (~4.7 GB) and will take a while. Please be patient.\n",
            "✅ Generation model downloaded successfully.\n",
            "\n",
            "--- Verifying all models are installed on the server... ---\n",
            "NAME                        ID              SIZE      MODIFIED               \n",
            "llama3:latest               365c0bd3c000    4.7 GB    Less than a second ago    \n",
            "mxbai-embed-large:latest    468836162de7    669 MB    About a minute ago        \n",
            "\n",
            "--- ✅ Step 3 Complete. Both models should be listed above. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the RAG Pipeline (The Setup)\n",
        "import os\n",
        "import time\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "print(\"--- Starting RAG Pipeline Setup ---\")\n",
        "\n",
        "# --- 1. Load the Document ---\n",
        "file_path = \"alice_in_wonderland.txt\"\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"FATAL ERROR: '{file_path}' not found. Please upload it and restart.\")\n",
        "else:\n",
        "    print(\"\\n--- 1. Loading Document ---\")\n",
        "    loader = TextLoader(file_path, encoding='utf-8')\n",
        "    documents = loader.load()\n",
        "\n",
        "    # --- 2. Split Document into Chunks ---\n",
        "    print(\"--- 2. Splitting Document into Chunks ---\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "    print(f\"Document split into {len(splits)} chunks.\")\n",
        "\n",
        "    # --- 3. Create Embeddings and Vector Store ---\n",
        "    print(\"--- 3. Creating Vector Store (this may take a moment)... ---\")\n",
        "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
        "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "    print(\"Vector Store created successfully.\")\n",
        "\n",
        "    # --- 4. Configure the RAG Chain ---\n",
        "    print(\"--- 4. Configuring the RAG Chain ---\")\n",
        "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8, \"fetch_k\": 20})\n",
        "    llm = ChatOllama(model=\"llama3\", temperature=0)\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "You must answer based ONLY on the provided context.\n",
        "If the answer is not contained within the text provided, you must say \"I cannot find that information in the provided text.\"\n",
        "Do not provide any information or commentary outside of the given context.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\"\"\")\n",
        "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "    print(\"\\n--- ✅ Step 4 Complete: RAG Pipeline is ready. You can now run the Q&A cell below. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfGCa2BggXVz",
        "outputId": "ccaf634b-a923-4aae-b4f5-234fa65757ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting RAG Pipeline Setup ---\n",
            "\n",
            "--- 1. Loading Document ---\n",
            "--- 2. Splitting Document into Chunks ---\n",
            "Document split into 195 chunks.\n",
            "--- 3. Creating Vector Store (this may take a moment)... ---\n",
            "Vector Store created successfully.\n",
            "--- 4. Configuring the RAG Chain ---\n",
            "\n",
            "--- ✅ Step 4 Complete: RAG Pipeline is ready. You can now run the Q&A cell below. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Questions with Debugging (The Interactive Testing Loop)\n",
        "\n",
        "# This cell uses the 'retrieval_chain' and 'retriever' variables created in the cell above.\n",
        "# You can run this cell multiple times without needing to rebuild the RAG pipeline.\n",
        "\n",
        "try:\n",
        "    retrieval_chain\n",
        "except NameError:\n",
        "    print(\"The 'retrieval_chain' is not defined. Please run the 'Build the RAG Pipeline' cell (Step 4) first.\")\n",
        "else:\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        query = input(\"Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): \")\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        if not query.strip():\n",
        "            continue\n",
        "\n",
        "        # DEBUG: See what the retriever finds\n",
        "        print(\"\\n--- DEBUG: Retrieving context... ---\")\n",
        "        retrieved_docs = retriever.invoke(query)\n",
        "        print(f\"Found {len(retrieved_docs)} chunks for the LLM.\")\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            print(f\"[CHUNK {i+1}]: {doc.page_content[:120]}...\")\n",
        "        print(\"--- END OF CONTEXT ---\")\n",
        "\n",
        "        # Get the final answer\n",
        "        print(\"\\n--- Generating final answer... ---\")\n",
        "        response = retrieval_chain.invoke({\"input\": query})\n",
        "\n",
        "        print(\"\\n\" + \"✅\" * 25)\n",
        "        print(f\"\\nFinal Answer:\\n\\n{response['answer']}\")\n",
        "        print(\"✅\" * 25)\n",
        "\n",
        "print(\"\\n--- Exiting Program. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKwPLGJrNRlU",
        "outputId": "cef89e1f-6aba-4b0b-f9c3-da15275077b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): Who is the first US president\n",
            "\n",
            "--- DEBUG: Retrieving context... ---\n",
            "Found 8 chunks for the LLM.\n",
            "[CHUNK 1]: `Read them,' said the King.\n",
            "\n",
            "  The White Rabbit put on his spectacles.  `Where shall I begin,\n",
            "please your Majesty?' he a...\n",
            "[CHUNK 2]: `And now which is which?' she said to herself, and nibbled a\n",
            "little of the right-hand bit to try the effect:  the next m...\n",
            "[CHUNK 3]: `In my youth,' Father William replied to his son,\n",
            "      `I feared it might injure the brain;\n",
            "    But, now that I'm perfe...\n",
            "[CHUNK 4]: `Ahem!' said the Mouse with an important air, `are you all ready?\n",
            "This is the driest thing I know.  Silence all round, i...\n",
            "[CHUNK 5]: CHAPTER VI\n",
            "\n",
            "                         Pig and Pepper\n",
            "\n",
            "\n",
            "  For a minute or two she stood looking at the house, and\n",
            "wonderin...\n",
            "[CHUNK 6]: `Fury said to a\n",
            "                   mouse, That he\n",
            "                 met in the\n",
            "               house,\n",
            "            \"Let us\n",
            "...\n",
            "[CHUNK 7]: The executioner's argument was, that you couldn't cut off a\n",
            "head unless there was a body to cut it off from:  that he ha...\n",
            "[CHUNK 8]: Alice had never been in a court of justice before, but she had\n",
            "read about them in books, and she was quite pleased to fi...\n",
            "--- END OF CONTEXT ---\n",
            "\n",
            "--- Generating final answer... ---\n",
            "\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "I cannot find that information in the provided text.\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): What animal was sitting and grinning in the Duchess’s kitchen?\n",
            "\n",
            "--- DEBUG: Retrieving context... ---\n",
            "Found 8 chunks for the LLM.\n",
            "[CHUNK 1]: `Anything you like,' said the Footman, and began whistling.\n",
            "\n",
            "  `Oh, there's no use in talking to him,' said Alice desper...\n",
            "[CHUNK 2]: `Found WHAT?' said the Duck.\n",
            "\n",
            "  `Found IT,' the Mouse replied rather crossly:  `of course you\n",
            "know what \"it\" means.'\n",
            "\n",
            "  ...\n",
            "[CHUNK 3]: Alice was just beginning to think to herself, `Now, what am I\n",
            "to do with this creature when I get it home?' when it grun...\n",
            "[CHUNK 4]: `Soo--oop of the e--e--evening,\n",
            "        Beautiful, beautiful Soup!'\n",
            "\n",
            "\n",
            "\n",
            "                           CHAPTER XI\n",
            "\n",
            "          ...\n",
            "[CHUNK 5]: `She's in prison,' the Queen said to the executioner:  `fetch\n",
            "her here.'  And the executioner went off like an arrow.\n",
            "\n",
            " ...\n",
            "[CHUNK 6]: The long grass rustled at her feet as the White Rabbit hurried\n",
            "by--the frightened Mouse splashed his way through the\n",
            "nei...\n",
            "[CHUNK 7]: `Please would you tell me,' said Alice, a little timidly, for\n",
            "she was not quite sure whether it was good manners for her...\n",
            "[CHUNK 8]: `If that's all you know about it, you may stand down,'\n",
            "continued the King.\n",
            "\n",
            "  `I can't go no lower,' said the Hatter:  `...\n",
            "--- END OF CONTEXT ---\n",
            "\n",
            "--- Generating final answer... ---\n",
            "\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "According to the provided context, a large cat was sitting on the hearth and grinning from ear to ear in the Duchess's kitchen.\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): exit\n",
            "\n",
            "--- Exiting Program. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Test (Clean Output)\n",
        "\n",
        "import time\n",
        "\n",
        "# This code assumes the 'retrieval_chain' variable was created in the previous cell.\n",
        "# It performs a single, non-interactive query and prints a clean result.\n",
        "\n",
        "# Safety check to ensure the RAG pipeline has been built.\n",
        "try:\n",
        "    retrieval_chain\n",
        "except NameError:\n",
        "    print(\"FATAL ERROR: The 'retrieval_chain' is not defined. Please run the 'Build the RAG Pipeline' cell first.\")\n",
        "else:\n",
        "    # 1. Define the single question you want to ask.\n",
        "    # You can change the text inside the quotes to test other questions.\n",
        "    question = \"Why did Alice decide not to drink immediately from the bottle labeled “DRINK ME”?\"\n",
        "\n",
        "    print(\"--- Running a single, clean test ---\")\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "\n",
        "    # 2. Invoke the chain to get the response and measure the time.\n",
        "    start_time = time.time()\n",
        "    response = retrieval_chain.invoke({\"input\": question})\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 3. Print only the final, clean answer.\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"Final Answer:\")\n",
        "    # Access the 'answer' key from the response dictionary.\n",
        "    print(response['answer'])\n",
        "    print(\"-\"*50)\n",
        "    print(f\"(Query completed in {end_time - start_time:.2f} seconds)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nIC2H_9Ort8",
        "outputId": "91d93b61-d530-4ec0-b2ad-e82ed41adcdb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running a single, clean test ---\n",
            "\n",
            "Question: Why did Alice decide not to drink immediately from the bottle labeled “DRINK ME”?\n",
            "\n",
            "--------------------------------------------------\n",
            "Final Answer:\n",
            "According to the provided context, Alice decided not to drink immediately from the bottle labeled \"DRINK ME\" because she had read several nice little histories about children who got burnt, eaten up by wild beasts and other unpleasant things, all because they WOULD not remember the simple rules their friends had taught them. She had never forgotten that if you drink much from a bottle marked \"poison,\" it is almost certain to disagree with you, sooner or later.\n",
            "--------------------------------------------------\n",
            "(Query completed in 4.75 seconds)\n"
          ]
        }
      ]
    }
  ]
}
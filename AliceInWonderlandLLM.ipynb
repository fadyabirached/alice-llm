{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sVYPTzgfgyN",
        "outputId": "7c1df05f-f757-44cc-ec56-c796f7bca98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Installing Python libraries... ---\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-ollama in /usr/local/lib/python3.11/dist-packages (0.3.6)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.12.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.13)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: ollama<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from langchain-ollama) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "\n",
            "--- 2. Installing Ollama... ---\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\n",
            "--- ✅ Step 1 Complete: All software installed. ---\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install All Libraries & Ollama\n",
        "print(\"--- 1. Installing Python libraries... ---\")\n",
        "!pip install -U langchain langchain-community langchain-ollama faiss-cpu\n",
        "\n",
        "print(\"\\n--- 2. Installing Ollama... ---\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"\\n--- ✅ Step 1 Complete: All software installed. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Start and Verify Ollama Server\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "print(\"--- 1. Stopping any old Ollama server processes (for a clean start)... ---\")\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"ollama\"], check=True)\n",
        "    print(\"Old Ollama processes stopped.\")\n",
        "    time.sleep(3)\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"No old Ollama processes found. Perfect.\")\n",
        "\n",
        "print(\"\\n--- 2. Starting a new Ollama server in the background... ---\")\n",
        "command = \"nohup ollama serve > ollama.log 2>&1 &\"\n",
        "os.system(command)\n",
        "print(\"Server start command issued. Waiting 20 seconds for it to initialize fully...\")\n",
        "# We wait a generous amount of time to ensure the server is ready.\n",
        "time.sleep(20)\n",
        "\n",
        "print(\"\\n--- 3. Verifying the server is running... ---\")\n",
        "# Check the log file for the \"Listening on\" message, which indicates success.\n",
        "print(\"--- Log File Content: ---\")\n",
        "!cat ollama.log\n",
        "\n",
        "# Check the system's process list to see if 'ollama serve' is an active process.\n",
        "print(\"\\n--- Active Processes: ---\")\n",
        "!ps -ef | grep ollama\n",
        "\n",
        "print(\"\\n--- ✅ Step 2 Complete. Check the output above. You should see 'Listening on...' in the log and 'ollama serve' in the process list. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Sd49Y6-f3iq",
        "outputId": "95174884-9213-4796-8ba1-f168854f5dfb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Stopping any old Ollama server processes (for a clean start)... ---\n",
            "No old Ollama processes found. Perfect.\n",
            "\n",
            "--- 2. Starting a new Ollama server in the background... ---\n",
            "Server start command issued. Waiting 20 seconds for it to initialize fully...\n",
            "\n",
            "--- 3. Verifying the server is running... ---\n",
            "--- Log File Content: ---\n",
            "time=2025-08-14T10:41:34.812Z level=INFO source=routes.go:1304 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-08-14T10:41:35.378Z level=INFO source=images.go:477 msg=\"total blobs: 0\"\n",
            "time=2025-08-14T10:41:35.378Z level=INFO source=images.go:484 msg=\"total unused blobs removed: 0\"\n",
            "time=2025-08-14T10:41:35.378Z level=INFO source=routes.go:1357 msg=\"Listening on 127.0.0.1:11434 (version 0.11.4)\"\n",
            "time=2025-08-14T10:41:35.378Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
            "time=2025-08-14T10:41:35.694Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-510ea78d-699d-35d6-3406-6805c513b3e6 library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
            "time=2025-08-14T10:41:35.694Z level=INFO source=routes.go:1398 msg=\"entering low vram mode\" \"total vram\"=\"14.7 GiB\" threshold=\"20.0 GiB\"\n",
            "\n",
            "--- Active Processes: ---\n",
            "root        7053       1  3 10:41 ?        00:00:00 ollama serve\n",
            "root        7147    6037  0 10:41 ?        00:00:00 /bin/bash -c ps -ef | grep ollama\n",
            "root        7149    7147  0 10:41 ?        00:00:00 grep ollama\n",
            "\n",
            "--- ✅ Step 2 Complete. Check the output above. You should see 'Listening on...' in the log and 'ollama serve' in the process list. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Pull Models using the Python Library\n",
        "import ollama\n",
        "\n",
        "print(\"--- Starting model downloads. This will take several minutes. ---\")\n",
        "\n",
        "try:\n",
        "    # Pull the smaller, faster embedding model first as a test\n",
        "    print(\"\\n--- Pulling embedding model (mxbai-embed-large)... ---\")\n",
        "    ollama.pull('mxbai-embed-large')\n",
        "    print(\"✅ Embedding model downloaded successfully.\")\n",
        "\n",
        "    # Pull the large Llama 3 model\n",
        "    print(\"\\n--- Pulling generation model (llama3)... ---\")\n",
        "    print(\"This is a large file (~4.7 GB) and will take a while. Please be patient.\")\n",
        "    ollama.pull('llama3')\n",
        "    print(\"✅ Generation model downloaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An error occurred during model pull: {e}\")\n",
        "    print(\"Please check the server log from Step 2 for more details.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Verifying all models are installed on the server... ---\")\n",
        "!ollama list\n",
        "\n",
        "print(\"\\n--- ✅ Step 3 Complete. Both models should be listed above. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgtnKJmZJq09",
        "outputId": "646ddb26-b578-4f95-c449-70133be6a578"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting model downloads. This will take several minutes. ---\n",
            "\n",
            "--- Pulling embedding model (mxbai-embed-large)... ---\n",
            "✅ Embedding model downloaded successfully.\n",
            "\n",
            "--- Pulling generation model (llama3)... ---\n",
            "This is a large file (~4.7 GB) and will take a while. Please be patient.\n",
            "✅ Generation model downloaded successfully.\n",
            "\n",
            "--- Verifying all models are installed on the server... ---\n",
            "NAME                        ID              SIZE      MODIFIED               \n",
            "llama3:latest               365c0bd3c000    4.7 GB    Less than a second ago    \n",
            "mxbai-embed-large:latest    468836162de7    669 MB    2 minutes ago             \n",
            "\n",
            "--- ✅ Step 3 Complete. Both models should be listed above. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Build the RAG Pipeline (The Setup)\n",
        "import os\n",
        "import time\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "print(\"--- Starting RAG Pipeline Setup ---\")\n",
        "\n",
        "# --- 1. Load the Document ---\n",
        "file_path = \"alice_in_wonderland.txt\"\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"FATAL ERROR: '{file_path}' not found. Please upload it and restart.\")\n",
        "else:\n",
        "    print(\"\\n--- 1. Loading Document ---\")\n",
        "    loader = TextLoader(file_path, encoding='utf-8')\n",
        "    documents = loader.load()\n",
        "\n",
        "    # --- 2. Split Document into Chunks ---\n",
        "    print(\"--- 2. Splitting Document into Chunks ---\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "    print(f\"Document split into {len(splits)} chunks.\")\n",
        "\n",
        "    # --- 3. Create Embeddings and Vector Store ---\n",
        "    print(\"--- 3. Creating Vector Store (this may take a moment)... ---\")\n",
        "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
        "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "    print(\"Vector Store created successfully.\")\n",
        "\n",
        "    # --- 4. Configure the RAG Chain ---\n",
        "    print(\"--- 4. Configuring the RAG Chain ---\")\n",
        "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8, \"fetch_k\": 20})\n",
        "    llm = ChatOllama(model=\"llama3\", temperature=0)\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "You must answer based ONLY on the provided context.\n",
        "If the answer is not contained within the text provided, you must say \"I cannot find that information in the provided text.\"\n",
        "Do not provide any information or commentary outside of the given context.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\"\"\")\n",
        "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "    print(\"\\n--- ✅ Step 4 Complete: RAG Pipeline is ready. You can now run the Q&A cell below. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfGCa2BggXVz",
        "outputId": "a2e41844-a46c-4ce8-c344-f82945340566"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting RAG Pipeline Setup ---\n",
            "\n",
            "--- 1. Loading Document ---\n",
            "--- 2. Splitting Document into Chunks ---\n",
            "Document split into 195 chunks.\n",
            "--- 3. Creating Vector Store (this may take a moment)... ---\n",
            "Vector Store created successfully.\n",
            "--- 4. Configuring the RAG Chain ---\n",
            "\n",
            "--- ✅ Step 4 Complete: RAG Pipeline is ready. You can now run the Q&A cell below. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Ask Questions with Debugging (The Interactive Testing Loop)\n",
        "\n",
        "# This cell uses the 'retrieval_chain' and 'retriever' variables created in the cell above.\n",
        "# You can run this cell multiple times without needing to rebuild the RAG pipeline.\n",
        "\n",
        "try:\n",
        "    retrieval_chain\n",
        "except NameError:\n",
        "    print(\"The 'retrieval_chain' is not defined. Please run the 'Build the RAG Pipeline' cell (Step 4) first.\")\n",
        "else:\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        query = input(\"Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): \")\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "        if not query.strip():\n",
        "            continue\n",
        "\n",
        "        # DEBUG: See what the retriever finds\n",
        "        print(\"\\n--- DEBUG: Retrieving context... ---\")\n",
        "        retrieved_docs = retriever.invoke(query)\n",
        "        print(f\"Found {len(retrieved_docs)} chunks for the LLM.\")\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            print(f\"[CHUNK {i+1}]: {doc.page_content[:120]}...\")\n",
        "        print(\"--- END OF CONTEXT ---\")\n",
        "\n",
        "        # Get the final answer\n",
        "        print(\"\\n--- Generating final answer... ---\")\n",
        "        response = retrieval_chain.invoke({\"input\": query})\n",
        "\n",
        "        print(\"\\n\" + \"✅\" * 25)\n",
        "        print(f\"\\nFinal Answer:\\n\\n{response['answer']}\")\n",
        "        print(\"✅\" * 25)\n",
        "\n",
        "print(\"\\n--- Exiting Program. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKwPLGJrNRlU",
        "outputId": "8cd7832c-4bf4-49d5-cac3-90852271e30e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): What was written on the cake Alice found inside the glass box?\n",
            "\n",
            "--- DEBUG: Retrieving context... ---\n",
            "Found 8 chunks for the LLM.\n",
            "[CHUNK 1]: Soon her eye fell on a little glass box that was lying under\n",
            "the table:  she opened it, and found in it a very small cak...\n",
            "[CHUNK 2]: `Then it ought to be Number One,' said Alice.\n",
            "\n",
            "  The King turned pale, and shut his note-book hastily.\n",
            "`Consider your ve...\n",
            "[CHUNK 3]: Suddenly she came upon a little three-legged table, all made of\n",
            "solid glass; there was nothing on it except a tiny golde...\n",
            "[CHUNK 4]: `I think I should understand that better,' Alice said very\n",
            "politely, `if I had it written down:  but I can't quite follo...\n",
            "[CHUNK 5]: `Beautiful Soup!  Who cares for fish,\n",
            "    Game, or any other dish?\n",
            "    Who would not give all else for two\n",
            "    Pennywort...\n",
            "[CHUNK 6]: `Wake up, Alice dear!' said her sister; `Why, what a long\n",
            "sleep you've had!'\n",
            "\n",
            "  `Oh, I've had such a curious dream!' sai...\n",
            "[CHUNK 7]: Alice looked all round the table, but there was nothing on it\n",
            "but tea.  `I don't see any wine,' she remarked.\n",
            "\n",
            "  `There ...\n",
            "[CHUNK 8]: Either the well was very deep, or she fell very slowly, for she\n",
            "had plenty of time as she went down to look about her an...\n",
            "--- END OF CONTEXT ---\n",
            "\n",
            "--- Generating final answer... ---\n",
            "\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "According to the provided context, what was written on the cake Alice found inside the glass box is:\n",
            "\n",
            "`EAT ME'\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): What happened to Alice after she drank from the “DRINK ME” bottle?\n",
            "\n",
            "--- DEBUG: Retrieving context... ---\n",
            "Found 8 chunks for the LLM.\n",
            "[CHUNK 1]: It was all very well to say `Drink me,' but the wise little\n",
            "Alice was not going to do THAT in a hurry.  `No, I'll look\n",
            "f...\n",
            "[CHUNK 2]: As she said these words her foot slipped, and in another\n",
            "moment, splash! she was up to her chin in salt water.  Her firs...\n",
            "[CHUNK 3]: It did so indeed, and much sooner than she had expected:\n",
            "before she had drunk half the bottle, she found her head pressi...\n",
            "[CHUNK 4]: Down, down, down.  There was nothing else to do, so Alice soon\n",
            "began talking again.  `Dinah'll miss me very much to-nigh...\n",
            "[CHUNK 5]: There seemed to be no use in waiting by the little door, so she\n",
            "went back to the table, half hoping she might find anoth...\n",
            "[CHUNK 6]: `Sure, it does, yer honour:  but it's an arm for all that.'\n",
            "\n",
            "  `Well, it's got no business there, at any rate:  go and t...\n",
            "[CHUNK 7]: Alice caught the baby with some difficulty, as it was a queer-\n",
            "shaped little creature, and held out its arms and legs in...\n",
            "[CHUNK 8]: Luckily for Alice, the little magic bottle had now had its full\n",
            "effect, and she grew no larger:  still it was very uncom...\n",
            "--- END OF CONTEXT ---\n",
            "\n",
            "--- Generating final answer... ---\n",
            "\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "According to the provided context, after drinking from the \"DRINK ME\" bottle, Alice began to grow and grew until she had to kneel down on the floor. She then continued growing, eventually putting one arm out of the window and one foot up the chimney. Eventually, she reached a point where she could no longer grow any taller and fell onto a heap of sticks and dry leaves.\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): Why did Alice decide not to drink immediately from the bottle labeled “DRINK ME”?\n",
            "\n",
            "--- DEBUG: Retrieving context... ---\n",
            "Found 8 chunks for the LLM.\n",
            "[CHUNK 1]: It was all very well to say `Drink me,' but the wise little\n",
            "Alice was not going to do THAT in a hurry.  `No, I'll look\n",
            "f...\n",
            "[CHUNK 2]: There seemed to be no use in waiting by the little door, so she\n",
            "went back to the table, half hoping she might find anoth...\n",
            "[CHUNK 3]: `Why with an M?' said Alice.\n",
            "\n",
            "  `Why not?' said the March Hare.\n",
            "\n",
            "  Alice was silent.\n",
            "\n",
            "  The Dormouse had closed its eyes...\n",
            "[CHUNK 4]: It did so indeed, and much sooner than she had expected:\n",
            "before she had drunk half the bottle, she found her head pressi...\n",
            "[CHUNK 5]: Either the well was very deep, or she fell very slowly, for she\n",
            "had plenty of time as she went down to look about her an...\n",
            "[CHUNK 6]: `I think I should understand that better,' Alice said very\n",
            "politely, `if I had it written down:  but I can't quite follo...\n",
            "[CHUNK 7]: Alice caught the baby with some difficulty, as it was a queer-\n",
            "shaped little creature, and held out its arms and legs in...\n",
            "[CHUNK 8]: The first question of course was, how to get dry again:  they\n",
            "had a consultation about this, and after a few minutes it ...\n",
            "--- END OF CONTEXT ---\n",
            "\n",
            "--- Generating final answer... ---\n",
            "\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "Final Answer:\n",
            "\n",
            "According to the provided context, Alice decided not to drink immediately from the bottle labeled \"DRINK ME\" because she had read several nice little histories about children who got burnt, eaten up by wild beasts and other unpleasant things, all because they WOULD not remember the simple rules their friends had taught them. She had never forgotten that if you drink much from a bottle marked \"poison,\" it is almost certain to disagree with you, sooner or later.\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "==================================================\n",
            "Ask a question about 'Alice in Wonderland' (or type 'exit' to quit): exit\n",
            "\n",
            "--- Exiting Program. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Test (Clean Output)\n",
        "\n",
        "import time\n",
        "\n",
        "# This code assumes the 'retrieval_chain' variable was created in the previous cell.\n",
        "# It performs a single, non-interactive query and prints a clean result.\n",
        "\n",
        "# Safety check to ensure the RAG pipeline has been built.\n",
        "try:\n",
        "    retrieval_chain\n",
        "except NameError:\n",
        "    print(\"FATAL ERROR: The 'retrieval_chain' is not defined. Please run the 'Build the RAG Pipeline' cell first.\")\n",
        "else:\n",
        "    # 1. Define the single question you want to ask.\n",
        "    # You can change the text inside the quotes to test other questions.\n",
        "    question = \"Which two animals did Alice mention that offended the Mouse?\"\n",
        "\n",
        "    print(\"--- Running a single, clean test ---\")\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "\n",
        "    # 2. Invoke the chain to get the response and measure the time.\n",
        "    start_time = time.time()\n",
        "    response = retrieval_chain.invoke({\"input\": question})\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 3. Print only the final, clean answer.\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"Final Answer:\")\n",
        "    # Access the 'answer' key from the response dictionary.\n",
        "    print(response['answer'])\n",
        "    print(\"-\"*50)\n",
        "    print(f\"(Query completed in {end_time - start_time:.2f} seconds)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nIC2H_9Ort8",
        "outputId": "99fdea83-ccbc-4df3-e35f-845e48730db6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running a single, clean test ---\n",
            "\n",
            "Question: Which two animals did Alice mention that offended the Mouse?\n",
            "\n",
            "--------------------------------------------------\n",
            "Final Answer:\n",
            "According to the provided context, Alice mentioned \"cats\" and Dinah (the cat) as the two animals that offended the Mouse.\n",
            "--------------------------------------------------\n",
            "(Query completed in 2.88 seconds)\n"
          ]
        }
      ]
    }
  ]
}